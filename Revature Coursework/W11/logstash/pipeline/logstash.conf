# Logstash Pipeline Configuration
# Receives logs from Filebeat, processes them, sends to Elasticsearch

###################### Input ######################
input {
  beats {
    port => 5044
  }
}

###################### Filter ######################
filter {
  # The JSON is already parsed by Filebeat, but we can do additional processing
  
  # Parse the timestamp from the log
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }
  
  # Add log level as a keyword for better filtering
  if [level] {
    mutate {
      copy => { "level" => "log_level" }
    }
  }
  
  # Extract logger name for filtering
  if [logger_name] {
    mutate {
      copy => { "logger_name" => "logger" }
    }
  }
  
  # Remove unnecessary fields to save storage
  mutate {
    remove_field => ["host", "agent", "ecs", "input", "log"]
  }
}

###################### Output ######################
output {
  # Send to Elasticsearch with daily index
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "api-logs-%{+YYYY.MM.dd}"
  }
  
  # Also output to console for debugging (comment out in production)
  stdout {
    codec => rubydebug
  }
}
